{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f244bd44-47c3-45b8-82ae-7c1ac07918ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\elwin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays_v1.py:37: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.4.1)\n",
      "  from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\n"
     ]
    }
   ],
   "source": [
    "# --- CELL: Importing Required Libraries ---\n",
    "# This cell imports all necessary libraries for building a Multiclass CNN Image Classifier.\n",
    "# - TensorFlow (tf): Core deep learning framework.\n",
    "# - Sequential: Used to build the CNN model by stacking layers linearly.\n",
    "# - ImageDataGenerator: For loading images from directories and applying real-time data augmentation.\n",
    "# - Conv2D: 2D Convolutional layer for extracting spatial features from images.\n",
    "# - MaxPooling2D: Reduces spatial dimensions by taking max value in each pooling window.\n",
    "# - Flatten: Converts 3D feature maps into a 1D vector for Dense layers.\n",
    "# - Dense: Fully connected layer for classification.\n",
    "# - Dropout: Regularization technique to prevent overfitting by randomly dropping neurons.\n",
    "# - BatchNormalization: Normalizes activations between layers for stable and faster training.\n",
    "# - Input: Defines the input shape of the model.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout,BatchNormalization,Input\n",
    "\n",
    "# OUTPUT: A SciPy version warning may appear - this is a compatibility notice between NumPy and SciPy\n",
    "# and does NOT affect the code. It can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99694e7c-aa3b-42bd-9f2d-2ee4f67d7417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 images belonging to 3 classes.\n",
      "Found 115 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# --- CELL: Setting Up Image Data Generators (Train & Validation) ---\n",
    "# This cell configures data loading and augmentation for multiclass classification (3 classes).\n",
    "#\n",
    "# Parameters:\n",
    "#   - img_size=(128,128): All images resized to 128x128 pixels.\n",
    "#   - batch_size=32: Images are processed in batches of 32.\n",
    "#\n",
    "# TRAINING DATA AUGMENTATION:\n",
    "#   - rescale=1./255: Normalizes pixel values from [0,255] to [0,1].\n",
    "#   - rotation_range=20: Random rotation up to 20 degrees.\n",
    "#   - zoom_range=0.2: Random zoom in/out by 20%.\n",
    "#   - width/height_shift_range=0.1: Random horizontal/vertical shifts by 10%.\n",
    "#   - horizontal_flip=True: Random horizontal flipping.\n",
    "#   - validation_split=0.2: Reserves 20% of training data for validation.\n",
    "#\n",
    "# class_mode='categorical': One-hot encoded labels for multiclass classification (3 classes).\n",
    "\n",
    "img_size=(128,128)\n",
    "batch_size=32\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_genrator= train_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\elwin\\OneDrive\\Desktop\\Multiclass cnn , car, plane\\train\",\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_genrator=train_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\elwin\\OneDrive\\Desktop\\Multiclass cnn , car, plane\\test\",\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# OUTPUT:\n",
    "# - Found 2400 training images belonging to 3 classes (e.g., airplanes, cars, ships).\n",
    "# - Found 115 validation images belonging to 3 classes.\n",
    "# The 3 classes are loaded from subdirectory names in the dataset folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71a9e5ef-9da6-4175-b11a-f583662915f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL: Defining the CNN Model Architecture for Multiclass Classification ---\n",
    "# This cell builds a Sequential CNN with 3 convolutional blocks and a classification head.\n",
    "#\n",
    "# Architecture breakdown:\n",
    "# BLOCK 1: Conv2D(32 filters, 3x3, ReLU) -> BatchNormalization -> MaxPooling2D(2x2)\n",
    "#   - Extracts low-level features (edges, textures) from 128x128x3 input images.\n",
    "# BLOCK 2: Conv2D(64 filters) -> BatchNormalization -> MaxPooling2D\n",
    "#   - Captures mid-level patterns (shapes, parts of objects).\n",
    "# BLOCK 3: Conv2D(128 filters) -> BatchNormalization -> MaxPooling2D\n",
    "#   - Learns high-level, class-specific features.\n",
    "# CLASSIFIER:\n",
    "#   - Flatten: Converts 3D feature maps to 1D vector (14x14x128 = 25,088 features).\n",
    "#   - Dense(128, ReLU): Fully connected layer for learning feature combinations.\n",
    "#   - Dropout(0.5): Drops 50% of neurons during training to prevent overfitting.\n",
    "#   - Dense(3, softmax): Output layer with 3 neurons (one per class) and softmax activation\n",
    "#     for multi-class probability distribution.\n",
    "\n",
    "model=Sequential([\n",
    "    Input(shape=(128,128,3)),\n",
    "    \n",
    "    Conv2D(32,(3,3),activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(64,(3,3),activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Conv2D(128,(3,3),activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3,activation='softmax')\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5fad173-905a-42d7-96b0-a3a5b191e459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │          \u001b[38;5;34m73,856\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │       \u001b[38;5;34m3,211,392\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │             \u001b[38;5;34m387\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,305,923</span> (12.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,305,923\u001b[0m (12.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,305,475</span> (12.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,305,475\u001b[0m (12.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- CELL: Compiling the Model and Displaying Architecture Summary ---\n",
    "# Compilation configures the model for training:\n",
    "#   - optimizer='adam': Adam optimizer with adaptive learning rate.\n",
    "#   - loss='categorical_crossentropy': Appropriate loss function for multi-class classification\n",
    "#     with one-hot encoded labels.\n",
    "#   - metrics=['accuracy']: Tracks accuracy during training.\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# OUTPUT (model.summary):\n",
    "# The model has 3 convolutional blocks (Conv2D + BatchNorm + MaxPool) and a Dense classifier.\n",
    "# Key observations:\n",
    "# - Conv layers increase filters: 32 -> 64 -> 128 (progressively deeper feature extraction).\n",
    "# - Spatial dimensions reduce: 126->63->61->30->28->14 through convolution and pooling.\n",
    "# - After Flatten: 25,088 features (14x14x128).\n",
    "# - Dense(128) layer has the most params: 3,211,392 (due to large flattened input).\n",
    "# - Output Dense(3) with softmax gives probability for each of the 3 classes.\n",
    "# - Total params: 3,305,923 (12.61 MB) | Trainable: 3,305,475 | Non-trainable: 448 (BatchNorm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2ccbfe6-20d4-43a5-8a20-451a71264299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 904ms/step - accuracy: 0.6492 - loss: 2.1842 - val_accuracy: 0.3304 - val_loss: 9.9661\n",
      "Epoch 2/16\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 619ms/step - accuracy: 0.7333 - loss: 0.7120 - val_accuracy: 0.4435 - val_loss: 15.3270\n",
      "Epoch 3/16\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 620ms/step - accuracy: 0.7987 - loss: 0.5919 - val_accuracy: 0.5217 - val_loss: 12.5937\n",
      "Epoch 4/16\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 607ms/step - accuracy: 0.8325 - loss: 0.5161 - val_accuracy: 0.5739 - val_loss: 4.6275\n",
      "Epoch 5/16\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 623ms/step - accuracy: 0.8292 - loss: 0.5142 - val_accuracy: 0.4087 - val_loss: 4.4718\n",
      "Epoch 6/16\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 620ms/step - accuracy: 0.8521 - loss: 0.4588 - val_accuracy: 0.4870 - val_loss: 2.1110\n",
      "Epoch 7/16\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 607ms/step - accuracy: 0.8483 - loss: 0.4435 - val_accuracy: 0.7826 - val_loss: 0.8778\n",
      "Epoch 8/16\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 618ms/step - accuracy: 0.8567 - loss: 0.4052 - val_accuracy: 0.7130 - val_loss: 0.8037\n",
      "Epoch 9/16\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 613ms/step - accuracy: 0.8754 - loss: 0.3626 - val_accuracy: 0.7652 - val_loss: 1.0126\n",
      "Epoch 10/16\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 611ms/step - accuracy: 0.8800 - loss: 0.3838 - val_accuracy: 0.8435 - val_loss: 0.5880\n",
      "Epoch 11/16\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 599ms/step - accuracy: 0.8804 - loss: 0.3781 - val_accuracy: 0.7391 - val_loss: 1.1523\n",
      "Epoch 12/16\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 603ms/step - accuracy: 0.8842 - loss: 0.3777 - val_accuracy: 0.7043 - val_loss: 0.9101\n",
      "Epoch 13/16\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 601ms/step - accuracy: 0.8921 - loss: 0.3208 - val_accuracy: 0.8348 - val_loss: 0.3413\n",
      "Epoch 14/16\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 608ms/step - accuracy: 0.8933 - loss: 0.3326 - val_accuracy: 0.9391 - val_loss: 0.2183\n",
      "Epoch 15/16\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 605ms/step - accuracy: 0.8950 - loss: 0.3090 - val_accuracy: 0.8870 - val_loss: 0.3460\n",
      "Epoch 16/16\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 598ms/step - accuracy: 0.9050 - loss: 0.2747 - val_accuracy: 0.8870 - val_loss: 0.3591\n"
     ]
    }
   ],
   "source": [
    "# --- CELL: Training the CNN Model ---\n",
    "# This cell trains the model on the vehicle/aircraft image dataset for 16 epochs.\n",
    "#   - train_genrator: Feeds augmented training images.\n",
    "#   - epochs=16: The model sees the full training data 16 times.\n",
    "#   - validation_data=val_genrator: Validates after each epoch to monitor generalization.\n",
    "# Training history is stored for plotting accuracy/loss curves later.\n",
    "\n",
    "history=model.fit(train_genrator,epochs=16,validation_data=val_genrator)\n",
    "\n",
    "# OUTPUT (Training Logs):\n",
    "# The model trained for 16 epochs (75 batches/epoch = 2400 images / 32 batch size).\n",
    "# Key observations:\n",
    "# - Epoch 1:  Train Acc = 64.92%, Val Acc = 33.04% (random-level performance initially)\n",
    "# - Epoch 7:  Train Acc = 84.83%, Val Acc = 78.26% (significant improvement)\n",
    "# - Epoch 14: Train Acc = 89.33%, Val Acc = 93.91% (best val accuracy)\n",
    "# - Epoch 16: Train Acc = 90.50%, Val Acc = 88.70% (final epoch)\n",
    "# - Validation loss shows high variance (fluctuating), suggesting the model struggles slightly\n",
    "#   with generalization on the small validation set (115 images).\n",
    "# - Training accuracy steadily improves from ~65% to ~90.5%.\n",
    "# - The model achieves reasonable performance but could benefit from more data or fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b873a87-ed9b-404a-8f78-29c956e07db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 209ms/step - accuracy: 0.8522 - loss: 0.3662\n",
      "Test Accuracy: 0.852173924446106\n"
     ]
    }
   ],
   "source": [
    "# --- CELL: Evaluating the Model on Validation/Test Data ---\n",
    "# This cell evaluates the trained model on the validation set to get the final test accuracy and loss.\n",
    "# model.evaluate() runs a forward pass on all validation images without updating weights.\n",
    "\n",
    "test_loss, test_acc = model.evaluate(val_genrator)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "# OUTPUT:\n",
    "# - Test Accuracy: 85.22% (0.852173924446106)\n",
    "# - Test Loss: 0.3662\n",
    "# The model correctly classifies ~85% of the validation images across 3 classes.\n",
    "# This is a decent result given the relatively small dataset (2400 train, 115 val images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69a3eed3-ef9f-4f10-841f-edcfcb7dc0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\n",
      "predicted : airplanes\n"
     ]
    }
   ],
   "source": [
    "# --- CELL: Making a Single Image Prediction ---\n",
    "# This cell loads a single test image (an aeroplane), preprocesses it, and predicts its class.\n",
    "#\n",
    "# Preprocessing steps:\n",
    "#   1. Load image and resize to 128x128 (matching training input size).\n",
    "#   2. Convert to numpy array using img_to_array.\n",
    "#   3. Normalize pixel values to [0,1] by dividing by 255.\n",
    "#   4. Expand dimensions: (128,128,3) -> (1,128,128,3) to create a batch of 1.\n",
    "#   5. model.predict() returns probability for each of the 3 classes.\n",
    "#   6. np.argmax() gets the index of the highest probability class.\n",
    "#   7. Maps the index to the class label using train_genrator.class_indices.\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "img=image.load_img(r\"C:\\Users\\elwin\\Downloads\\aeroplane image.jpeg\",target_size=(128,128))\n",
    "img_array=image.img_to_array(img)\n",
    "img_array=img_array/255\n",
    "img_array=np.expand_dims(img_array,axis=0)\n",
    "\n",
    "prediction=model.predict(img_array)\n",
    "predicted_class=np.argmax(prediction)\n",
    "\n",
    "labels=list(train_genrator.class_indices.keys())\n",
    "print('predicted :',labels[predicted_class])\n",
    "\n",
    "# OUTPUT:\n",
    "# predicted : airplanes\n",
    "# The model correctly identified the uploaded image as an 'airplane' class.\n",
    "# This confirms the model is working as expected for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59963ea4-6c4b-41e4-8f42-97ed69a77202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL: Importing Gradio and PIL for Web Interface ---\n",
    "# - Gradio (gr): Library for creating interactive ML web UIs.\n",
    "# - PIL (Image): For image format conversion during preprocessing.\n",
    "\n",
    "import gradio as gr\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e623cdd9-7bcd-4ce5-80e6-58fff70f59c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELL: Setting Up the Gradio Web Interface for Vehicle Classification ---\n",
    "# This cell creates a Gradio-based web app for real-time vehicle/aircraft classification.\n",
    "#\n",
    "# Prediction function (vechicle_detection):\n",
    "#   1. Converts uploaded numpy image to PIL Image.\n",
    "#   2. Resizes to 128x128 (matching training size).\n",
    "#   3. Normalizes pixel values to [0,1].\n",
    "#   4. Expands dimensions to add batch axis.\n",
    "#   5. Runs prediction through the model.\n",
    "#   6. np.argmax() gets the class with highest probability.\n",
    "#   7. Maps index to class label (e.g., 'airplanes', 'cars', 'ships').\n",
    "#\n",
    "# Gradio Interface:\n",
    "#   - Input: Image upload widget.\n",
    "#   - Output: Textbox showing predicted vehicle class.\n",
    "\n",
    "models=model\n",
    "\n",
    "# image preprocessing funtion\n",
    "def vechicle_detection(image):\n",
    "    image=Image.fromarray(image)\n",
    "    image=image.resize((128,128))\n",
    "    image=np.array(image)/255.0\n",
    "    image=np.expand_dims(image,axis=0)\n",
    "\n",
    "    predictions=models.predict(image)\n",
    "    predict_class=np.argmax(predictions)\n",
    "    label=list(train_genrator.class_indices.keys())\n",
    "    return label[predict_class]\n",
    "        \n",
    "\n",
    "interface=gr.Interface(\n",
    "    fn=vechicle_detection,\n",
    "    inputs=gr.Image(type='numpy',label='Upload Image'),\n",
    "    outputs=gr.Textbox(label='Prediction'),\n",
    "    title='Vechicle image Classifier',\n",
    "    description='upload an image to identify the animal'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fbd75ed-3f52-42ff-8060-6a4147b8a97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n"
     ]
    }
   ],
   "source": [
    "# --- CELL: Launching the Gradio Web Application ---\n",
    "# This launches the Gradio interface as a local web server for the vehicle classifier.\n",
    "\n",
    "interface.launch()\n",
    "\n",
    "# OUTPUT: The Gradio app runs on http://127.0.0.1:7860\n",
    "# An interactive iframe is embedded in the notebook.\n",
    "# When images are uploaded, the model predicts the vehicle type (airplane/car/ship).\n",
    "# Prediction runs in ~50ms per image (1/1 batch).\n",
    "# To share publicly, use interface.launch(share=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c95a31-f8e1-44e2-8634-e58e55df5cf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}